{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aamir_syed/nova/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-08-19 09:25:09,608\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "/home/aamir_syed/nova/venv/lib/python3.10/site-packages/pydantic/_internal/_fields.py:161: UserWarning: Field \"model_id\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"LITELLM_BASE_URL\"] = \"http://192.168.0.105:4000\"\n",
    "os.environ[\"LITELLM_API_KEY\"] = \"sk-235bfb24-7434-4223-84fc-6cfb88c74ec9\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2,3\"\n",
    "\n",
    "from concept_extractor.text_chunker import SimpleChunker, LineChunker\n",
    "from concept_extractor.concept_extractor import ConceptExtractor, ConceptValidator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from uuid import uuid4\n",
    "from typing import Any, Union, List, Dict\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from nova.contextual_retrievers import DocumentRetriever\n",
    "from concept_extractor.text_chunker import LineChunker\n",
    "\n",
    "def simple_blitz(texts:List[str],\n",
    "                 encoding_model:SentenceTransformer,\n",
    "                 threshold:float=0.6) -> pd.DataFrame:\n",
    "\n",
    "    text_embeddings = encoding_model.encode(texts)\n",
    "    sim_matrix = cosine_similarity(text_embeddings, text_embeddings)\n",
    "\n",
    "    communities = pd.DataFrame(columns=[\"node\", \"cls\"])\n",
    "\n",
    "    #secret sauce - soft mini-blitz clustering\n",
    "    for idx, row in enumerate(sim_matrix):\n",
    "\n",
    "        connected_items = np.where(row >= threshold)[0]\n",
    "        if len(connected_items) < 3:\n",
    "            continue\n",
    "\n",
    "        sim_matrix[connected_items, connected_items] = 0.\n",
    "\n",
    "        # sim_matrix[:, connected_items] = 0.\n",
    "        # sim_matrix[connected_items, :] = 0.\n",
    "\n",
    "        simple_community = pd.DataFrame({\"node\" : [texts[_] for _ in connected_items]})\n",
    "        simple_community[\"cls\"] = str(uuid4())\n",
    "\n",
    "        communities = pd.concat([communities, simple_community], axis=0, ignore_index=True)\n",
    "\n",
    "    #enable multi-label soft clustering\n",
    "    text_nodes = pd.DataFrame({\"node\" : texts})\n",
    "    unclustered_communities = text_nodes[~text_nodes[\"node\"].isin(communities[\"node\"])].copy()\n",
    "    unclustered_communities[\"cls\"] = \"-1\"\n",
    "\n",
    "    communities = pd.concat([communities,\n",
    "                             unclustered_communities],\n",
    "                             axis=0, ignore_index=True)\n",
    "\n",
    "    return communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import networkx as nx\n",
    "\n",
    "import spacy\n",
    "\n",
    "from typing import Dict, List\n",
    "\n",
    "from utils.llm_pipeline import AsyncLLMPipelineBuilder\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "class GraphIndexRetriever(DocumentRetriever):\n",
    "\n",
    "    def __init__(self,\n",
    "                 config:dict={},\n",
    "                 logger:logging.Logger=None):\n",
    "        \n",
    "        super().__init__(config=config, logger=logger)\n",
    "\n",
    "        self.concepts = None\n",
    "        self.valid_concepts = None\n",
    "\n",
    "        self.connected_nodes = pd.DataFrame(columns=[\"node\", \"originating_texts\", \"summary\"])\n",
    "        self.communities = pd.DataFrame(columns=[\"node\", \"cls\", \"type\", \"originating_texts\", \"summary\"])\n",
    "\n",
    "        self.graph_context = None\n",
    "        self.sentence_window_separator = \"\\n [SEP] \\n\"\n",
    "        \n",
    "        self.encoding_model = SentenceTransformer(\"paraphrase-MiniLM-L6-v2\")\n",
    "\n",
    "    def _extract_keywords(self, documents: List[str]) -> List[Dict[str, Union[int, str, List[str]]]]:\n",
    "        \n",
    "        keywords = []\n",
    "\n",
    "        if isinstance(documents, str):\n",
    "            documents = [documents]\n",
    "\n",
    "        for idx, doc in enumerate(nlp.pipe(documents, batch_size=128)):\n",
    "\n",
    "            entities = [ent.text for ent in doc.ents]\n",
    "            noun_chunks = [chunk.text for chunk in doc.noun_chunks if len(chunk.text.split()) > 1]\n",
    "\n",
    "            doc_keywords = list(set(entities + noun_chunks))\n",
    "            \n",
    "            #get only the keywords which are not substrings of other keywords\n",
    "            doc_keywords = [keyword for keyword in doc_keywords\n",
    "                            if len([kw for kw in doc_keywords if kw != keyword and keyword in kw]) == 0]\n",
    "\n",
    "            keywords.append({\"index\" : idx, \"document\" : doc.text, \"keywords\" : doc_keywords})\n",
    "            \n",
    "        return keywords\n",
    "\n",
    "    def _get_keywords(self, document:str, concepts:List[Dict[str, Any]]) -> List[str]:\n",
    "        \n",
    "        keywords = []\n",
    "\n",
    "        try:\n",
    "            concepts = pd.DataFrame(concepts)\n",
    "            \n",
    "            #keywords are just nodes in the graph -- either a source node (node_1) or a target node (node_2)\n",
    "            graph_lookup = concepts[concepts[\"originating_text\"] == document]\n",
    "            keywords = graph_lookup[\"node_1\"].tolist() + graph_lookup[\"node_2\"].tolist()\n",
    "\n",
    "            keywords = list(set(keywords))\n",
    "            keywords = [{\"entity\": keyword,\n",
    "                         \"score\" : 1.0,\n",
    "                         \"extraction_type\" : \"concept_graph\"}\n",
    "                         for keyword in keywords]\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        return keywords\n",
    "\n",
    "    def _build_graph_context(self, graph:pd.DataFrame) -> str:\n",
    "\n",
    "        if self.graph_context is None:\n",
    "\n",
    "            self.summarizer = AsyncLLMPipelineBuilder(system_prompt=\"summarize the following texts. Be informative in your summary and mention key points/ideas. Keep the summary to 5 lines or less.\",\n",
    "                                                        model=\"gemini-1.5-flash\")\n",
    "            \n",
    "            #limit to 500 edges\n",
    "            self.graph_context = self.summarizer.batch_predict([\", \".join(graph['edge'].tolist()[:500])])[-1]\n",
    "        \n",
    "        return self.graph_context\n",
    "    \n",
    "    def _find_connected_nodes(self, concepts:List[Dict[str, Any]]) -> pd.DataFrame:\n",
    "\n",
    "        concepts = pd.DataFrame(concepts)\n",
    "        G = nx.from_pandas_edgelist(concepts, 'node_1', 'node_2', edge_attr='weight')\n",
    "\n",
    "        _min_connections = max(3, len(concepts)//1000) #minimum number of connections to be considered 'connected'\n",
    "        connected_nodes = [node for node in G.nodes() if G.degree[node] >= _min_connections]\n",
    "        connected_nodes = pd.DataFrame(connected_nodes,\n",
    "                                    columns=[\"node\"]).drop_duplicates(subset=[\"node\"])\n",
    "        \n",
    "        connected_nodes[\"originating_texts\"] = connected_nodes[\"node\"].apply(lambda x: list(set(concepts[concepts[\"node_1\"] == x][\"originating_text\"].unique().tolist() + concepts[concepts[\"node_2\"] == x][\"originating_text\"].unique().tolist())))\n",
    "        connected_nodes[\"originating_texts\"] = connected_nodes[\"originating_texts\"].apply(lambda x: self.sentence_window_separator.join(x))\n",
    "\n",
    "        self.logger.info(f\"generating summaries for {connected_nodes.shape[0]} connected nodes ...\")\n",
    "\n",
    "        summaries_to_generate = [f\"summarize the following texts around ideas/opinions related to {row['node']}:\\n{row['originating_texts']}\"\n",
    "                                for _, row in connected_nodes.iterrows()]\n",
    "        \n",
    "        summaries = self.summarizer.batch_predict(summaries_to_generate)\n",
    "        connected_nodes[\"summary\"] = summaries\n",
    "        \n",
    "        return connected_nodes\n",
    "\n",
    "    def _find_communities(self, concepts:List[Dict[str, Any]]) -> pd.DataFrame:\n",
    "\n",
    "        concepts = pd.DataFrame(concepts)\n",
    "\n",
    "        if concepts.empty:\n",
    "            return concepts\n",
    "        \n",
    "        entities_only_communities = concepts[\"node_1\"].unique().tolist() + concepts[\"node_2\"].unique().tolist()\n",
    "        entities_only_communities = list(set(entities_only_communities))\n",
    "        self.logger.info(f'number of entities : {len(entities_only_communities)}')\n",
    "\n",
    "        edge_communities = concepts[\"edge\"].unique().tolist()\n",
    "        self.logger.info(f'number of edges : {len(edge_communities)}')\n",
    "\n",
    "        relation_only_communities = [f\"{row['node_1']} {row['relation']} {row['node_2']}\"\n",
    "                                    for _, row in concepts.iterrows()]\n",
    "        relation_only_communities = list(set(relation_only_communities))\n",
    "        \n",
    "        self.logger.info(f'number of relations : {len(relation_only_communities)}')\n",
    "\n",
    "        entities_only_communities = simple_blitz(entities_only_communities,\n",
    "                                                self.encoding_model,\n",
    "                                                threshold=0.7)\n",
    "        entities_only_communities[\"type\"] = \"similar_nodes\"\n",
    "        entities_only_communities[\"originating_texts\"] = entities_only_communities[\"node\"].apply(lambda x: list(set(concepts[concepts[\"node_1\"] == x][\"originating_text\"].unique().tolist() + concepts[concepts[\"node_2\"] == x][\"originating_text\"].unique().tolist())))\n",
    "        entities_only_communities[\"originating_texts\"] = entities_only_communities[\"originating_texts\"].apply(lambda x: \"\\n\".join(x))\n",
    "        \n",
    "        edge_communities = simple_blitz(edge_communities,\n",
    "                                        self.encoding_model,\n",
    "                                        threshold=0.85)\n",
    "        edge_communities[\"type\"] = \"similar_edges\"\n",
    "        edge_communities[\"originating_texts\"] = edge_communities[\"node\"].apply(lambda x: concepts[concepts[\"edge\"] == x][\"originating_text\"].unique().tolist())\n",
    "        edge_communities[\"originating_texts\"] = edge_communities[\"originating_texts\"].apply(lambda x: \"\\n\".join(x))\n",
    "\n",
    "        relation_only_communities = simple_blitz(relation_only_communities,\n",
    "                                                self.encoding_model,\n",
    "                                                threshold=0.9)\n",
    "        relation_only_communities[\"type\"] = \"similar_relations\"\n",
    "        relation_only_communities[\"originating_texts\"] = relation_only_communities[\"node\"]\n",
    "            \n",
    "        communities = pd.concat([entities_only_communities[entities_only_communities[\"cls\"] != \"-1\"],\n",
    "                                edge_communities[edge_communities[\"cls\"] != \"-1\"],\n",
    "                                relation_only_communities[relation_only_communities[\"cls\"] != \"-1\"]\n",
    "                                ],\n",
    "                                axis=0, ignore_index=True)\n",
    "            \n",
    "        #get cls level summaries\n",
    "        # unique_clusters = communities.drop_duplicates(subset=[\"cls\"])\n",
    "        # unique_clusters[\"group_summaries\"] = unique_clusters[\"cls\"].apply(lambda x : communities[communities[\"cls\"] == x][\"originating_texts\"].tolist())\n",
    "\n",
    "        # print(unique_clusters.head(), unique_clusters.shape)\n",
    "        # communities_to_summarize = [f\"summarize the following texts around ideas/opinions related to the keywords {communities[communities['cls'] == row['cls']]['node'].unique().tolist()}:\\n\\n{', '.join(row['group_summaries'])}\"\n",
    "        #                             for idx, row in unique_clusters.iterrows()]\n",
    "        \n",
    "        # cls_summaries = self.summarizer.batch_predict(communities_to_summarize)\n",
    "        # unique_clusters[\"summary\"] = cls_summaries\n",
    "\n",
    "        # communities = pd.merge(communities,\n",
    "        #                         unique_clusters[[\"cls\", \"summary\"]],\n",
    "        #                         on=\"cls\",\n",
    "        #                         how=\"left\")\n",
    "        \n",
    "        # communities.reset_index(inplace=True, drop=True)\n",
    "        communities[\"summary\"] = communities[\"node\"].tolist()\n",
    "\n",
    "        return communities\n",
    "    \n",
    "    def search_nodes(self,\n",
    "                    query:str,\n",
    "                    graph_context:str,\n",
    "                    node_subgraph:pd.DataFrame) -> List[Dict[str, Union[str, List[str]]]]:\n",
    "        \"\"\"\n",
    "        'search' nodes that are frequently mentioned, reason which of the nodes have relevant information for the query, summarize information\n",
    "\n",
    "        Args:\n",
    "            - query : str : the query to search for\n",
    "            - graph_context : str : the summary of the graph\n",
    "            - node_subgraph : pd.DataFrame : the subgraph of the nodes\n",
    "            - index_builder : DocumentRetriever : the index builder object\n",
    "\n",
    "        Returns:\n",
    "            - relevant_to_query : List[Dict[str, Union[str, List[str]]] : the relevant nodes to the query\n",
    "        \"\"\"\n",
    "        \n",
    "        extracted_entity_keywords = self._extract_keywords(query)[0][\"keywords\"]\n",
    "        extracted_entity_keywords = [{\"node_1\" : entity, \"relation\" : query, \"node_2\" : entity, \"edge\" : query}\n",
    "                                for entity in extracted_entity_keywords]\n",
    "        self.logger.info(f\"Extracted entities and relations from the query : {extracted_entity_keywords}\\n\")\n",
    "        \n",
    "        try:\n",
    "\n",
    "            er_query = f\"\"\"you are an expert analyst working on a transcript. A summary of the transcript is as below:\\n{graph_context}\\n\n",
    "                        Given this context, extract the entities and relationships between them as a list of dictionaries. each dictionary\n",
    "                        should have the keys 'node_1', 'relation', 'node_2', 'edge'.\"\"\"\n",
    "        \n",
    "            graph_query_builder = AsyncLLMPipelineBuilder(system_prompt=er_query,\n",
    "                                                        model=\"gemini-1.5-flash\")\n",
    "            query_entity_relations = graph_query_builder.batch_predict([f\"only return a list of dictionaries with the keys 'node_1', 'relation', 'node_2', 'edge' for the following query : {query}\"])[0]\n",
    "            query_entity_relations = query_entity_relations.replace(\"\\n\", \" \").replace(\"'\", \"\").replace(\"`\", \"\").replace(\"json\", \"\").replace(\"python\", \"\").strip()\n",
    "\n",
    "\n",
    "            self.logger.info(f\"Expanded query : {query_entity_relations}\")\n",
    "            query_entity_relations = ast.literal_eval(query_entity_relations)\n",
    "            query_entity_relations += extracted_entity_keywords\n",
    "        \n",
    "        except:\n",
    "            query_entity_relations = extracted_entity_keywords\n",
    "\n",
    "        if query_entity_relations == []:\n",
    "            return pd.DataFrame(columns=node_subgraph.columns).to_dict('records')\n",
    "        \n",
    "        entities_in_query = pd.DataFrame(query_entity_relations)[\"node_1\"].tolist() + pd.DataFrame(query_entity_relations)[\"node_2\"].tolist()\n",
    "\n",
    "        entities_in_query = list(set(entities_in_query))\n",
    "\n",
    "        relevant_to_query = node_subgraph.copy()\n",
    "\n",
    "        #find entities that are in the node-subgraph that might be relevant to the query\n",
    "        query_entity_embeddings = self.encoding_model.encode(entities_in_query)\n",
    "        node_subgraph_embeddings = self.encoding_model.encode(node_subgraph[\"node\"].tolist())\n",
    "        query2node_sim_matrix = cosine_similarity(query_entity_embeddings, node_subgraph_embeddings)\n",
    "\n",
    "        similar_subgraph_nodes = []\n",
    "        for row in query2node_sim_matrix:\n",
    "            connected_items = np.where(row >= 0.7)[0]\n",
    "            similar_subgraph_nodes.extend(list(connected_items))\n",
    "\n",
    "        similar_subgraph_nodes = list(set(similar_subgraph_nodes))\n",
    "        similar_subgraph_nodes = node_subgraph.iloc[similar_subgraph_nodes][\"node\"].tolist()\n",
    "\n",
    "        self.logger.info(f\"found {len(similar_subgraph_nodes)} nodes in the subgraph that are similar to the query entities ...\")\n",
    "        self.logger.info(f\"similar nodes : {similar_subgraph_nodes}\\n\")\n",
    "\n",
    "        #get the originating texts for these nodes\n",
    "        relevant_to_query = relevant_to_query[relevant_to_query[\"node\"].isin(similar_subgraph_nodes)]\n",
    "        relevant_to_query[\"type\"] = \"node_search\"\n",
    "\n",
    "        if relevant_to_query.empty:\n",
    "            self.logger.info(f\"no relevant nodes found in the subgraph for the query\")\n",
    "\n",
    "        return relevant_to_query.to_dict('records')\n",
    "    \n",
    "    def search_communities(self,\n",
    "                           query:str,\n",
    "                           graph_context:str,\n",
    "                           communities:pd.DataFrame) -> List[Dict[str, Union[str, List[str]]]]:\n",
    "        \"\"\"\n",
    "        search the communities formed in the graph for relevant information.\n",
    "\n",
    "        - get relevant keywords to 'answer' a query using the graph context\n",
    "        - search the community nodes for relevant information\n",
    "        - for every unique 'cls' of the community, retrieve all 'connected' nodes\n",
    "\n",
    "        Args:\n",
    "            - query : str : the query to search for\n",
    "            - graph_context : str : the summary of the graph\n",
    "            - communities : pd.DataFrame : the communities formed in the graph\n",
    "\n",
    "        Returns:\n",
    "            - connected_nodes : List[Dict[str, Union[str, List[str]]] : the connected nodes in the communities\n",
    "        \"\"\"\n",
    "\n",
    "        #simple query expansion -- ask an LLM what 'keywords' are required to answer the query\n",
    "        query_expansion_prompt = f\"you are an expert analyst working on a transcript. A summary of the transcript is as below:\\n{graph_context}\\n\"\n",
    "        with AsyncLLMPipelineBuilder(query_expansion_prompt, model=\"gemini-1.5-flash\") as llm:\n",
    "            expanded_query = llm.batch_predict([f\"give keywords relevant to answering '{query}'. only return a single string of keywords, separated by commas\"])[-1]\n",
    "\n",
    "        try:\n",
    "            expanded_query = expanded_query.replace(\"\\n\", \" \").replace(\"'\", \"\").replace(\"`\", \"\").replace(\"json\", \"\").replace(\"python\", \"\").replace(\"indeterminate\", \"\").strip()\n",
    "            expanded_query = expanded_query.split(\",\")\n",
    "        except:\n",
    "            expanded_query = []\n",
    "\n",
    "        if expanded_query == [] or expanded_query == ['']:\n",
    "            return pd.DataFrame(columns=communities.columns).to_dict('records')\n",
    "        \n",
    "        self.logger.info(f\"expanded query keywords : {expanded_query}\")\n",
    "        \n",
    "        #search the communities for relevant information\n",
    "        query_embeddings = self.encoding_model.encode(expanded_query)\n",
    "        community_embeddings = self.encoding_model.encode(communities[\"node\"].tolist())\n",
    "        query2community_sim_matrix = cosine_similarity(query_embeddings, community_embeddings)\n",
    "\n",
    "        relevant_communities = []\n",
    "        for row in query2community_sim_matrix:\n",
    "            connected_items = np.where(row >= 0.8)[0]\n",
    "            relevant_communities.extend(list(connected_items))\n",
    "\n",
    "        relevant_communities = list(set(relevant_communities))\n",
    "        relevant_communities = [communities[\"node\"].tolist()[rel_idx]\n",
    "                                for rel_idx in relevant_communities]\n",
    "        \n",
    "        #for every connected node, get the cluster it belongs to\n",
    "        connected_nodes = communities[communities[\"node\"].isin(relevant_communities)][\"cls\"].unique().tolist()\n",
    "        connected_nodes = communities[communities[\"cls\"].isin(connected_nodes)]\n",
    "        connected_nodes[\"score\"] = 0.8\n",
    "\n",
    "        connected_nodes.drop_duplicates(subset=[\"node\", \"cls\"], inplace=True)\n",
    "\n",
    "        return connected_nodes.to_dict('records')\n",
    "    \n",
    "    def _search_index(self, query:str):\n",
    "        #simple semantic search in the index and community search\n",
    "        pass\n",
    "\n",
    "    def build_index(self, documents: List[str]) -> None:\n",
    "\n",
    "        if self.index is None:\n",
    "\n",
    "            self.logger.info(f\"Building a graph-index using LLMs for {len(documents)} documents ...\")\n",
    "\n",
    "            data_chunks = LineChunker(docs=documents).get_output()\n",
    "            self.logger.info(f\"Chunked {len(documents)} documents into {len(data_chunks)} chunks ...\\n\")\n",
    "            \n",
    "            concept_extractor = ConceptExtractor(chunks=data_chunks, comprehensive=False)\n",
    "            self.concepts = concept_extractor.get_output()\n",
    "\n",
    "            self.logger.info(f\"Extracted {len(self.concepts)} concepts from {len(documents)} documents ...\\n\")\n",
    "\n",
    "            validator = ConceptValidator(concepts=self.concepts)\n",
    "            self.valid_concepts = validator.validate()\n",
    "\n",
    "            self.logger.info(f\"Found {len(self.valid_concepts)} valid concepts from {len(documents)} documents ...\\n\")\n",
    "\n",
    "            self.index = [{\"index\": idx,\n",
    "                        \"document\": doc,\n",
    "                        \"keywords\": self._get_keywords(doc, self.valid_concepts)}\n",
    "                        for idx, doc in enumerate(documents)]\n",
    "            \n",
    "            self.logger.info(f\"Built index for {len(documents)} documents | {len(self.index)} keyword pairs extracted\")\n",
    "            self.logger.info(f\"Building inverse index ...\")\n",
    "\n",
    "            #build graph context -- this is a summary of the graph.\n",
    "            self._build_graph_context(pd.DataFrame(self.valid_concepts))\n",
    "\n",
    "            self.logger.info(f\"Finding connected nodes ...\")\n",
    "            self.connected_nodes = self._find_connected_nodes(self.valid_concepts)\n",
    "\n",
    "            self.logger.info(f\"Finding communities ...\")\n",
    "            self.communities = self._find_communities(self.valid_concepts)\n",
    "\n",
    "            self.inverse_index = self.build_inverse_index(self.index)\n",
    "            self.logger.info(f\"Built inverse index for {len(self.inverse_index)} keywords\\n\")\n",
    "        \n",
    "    def retrieve_topk(self, query: str, topk: int=5) -> List[Dict[str, Union[int, str, List[str]]]]:\n",
    "\n",
    "        relevant_nodes, relevant_docs = [], []\n",
    "\n",
    "        # relevant_nodes = self.search_nodes(query,\n",
    "        #                                   graph_context=self.graph_context,\n",
    "        #                                   node_subgraph=self.connected_nodes)\n",
    "        \n",
    "        relevant_nodes += self.search_communities(query,\n",
    "                                                graph_context=self.graph_context,\n",
    "                                                communities=self.communities)\n",
    "        if relevant_nodes == []:\n",
    "            return pd.DataFrame(columns=['index', 'document', 'node', 'type', 'keywords'])\n",
    "        \n",
    "        relevant_nodes = pd.merge(pd.DataFrame(relevant_nodes)[[\"originating_texts\", \"node\", \"type\"]].rename(columns={\"originating_texts\" : \"document\"}),\n",
    "                                pd.DataFrame(self.index)[[\"document\", \"keywords\", \"index\"]],\n",
    "                                on=\"document\",\n",
    "                                how=\"left\").dropna(subset=[\"index\"]).drop_duplicates(subset=[\"document\", \"index\"])\n",
    "        relevant_nodes['index'] = relevant_nodes['index'].astype(int)\n",
    "        \n",
    "        # summary_docs = [{'index' : 'aux_doc',\n",
    "        #                  'document' : rel_document.get('summary', ''),\n",
    "        #                  'keywords' : [],\n",
    "        #                  'score' : 0.5}\n",
    "        #                  for rel_document in relevant_nodes]\n",
    "        # summary_docs = [doc for doc in summary_docs if doc.get('document') != '']\n",
    "        \n",
    "        #get all documents associated with the node -- overkill for number of documents retrieved\n",
    "\n",
    "        # documents_for_nodes = [self.inverse_index.get(keyword['node'], [])\n",
    "        #                        for keyword in relevant_nodes]\n",
    "        # documents_for_nodes = [arr for sublist in documents_for_nodes for arr in sublist]\n",
    "        # documents_for_nodes = [self.index[document_idx] for document_idx in documents_for_nodes]\n",
    "\n",
    "        # documents_for_nodes += summary_docs\n",
    "\n",
    "        # relevant_docs += self.search_communities(query, topk=topk)\n",
    "\n",
    "        documents_for_nodes = relevant_nodes.to_dict('records')\n",
    "\n",
    "        relevant_docs += documents_for_nodes\n",
    "        \n",
    "        return relevant_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_id</th>\n",
       "      <th>request_id</th>\n",
       "      <th>sequence_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MINI REVIEW ARTICLE\\n</td>\n",
       "      <td>e951f600-99e5-499c-8e45-78b87ad76291</td>\n",
       "      <td>med_research</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>published: 09 November 2011\\n</td>\n",
       "      <td>39e9fc72-cab9-48a2-8c92-fbca710d0e0a</td>\n",
       "      <td>med_research</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>doi: 10.3389/fphar.2011.00072\\n</td>\n",
       "      <td>d4025a7c-18ba-46d0-8659-356942573265</td>\n",
       "      <td>med_research</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Acetaminophen: beyond pain and fever-relieving\\n</td>\n",
       "      <td>f9092f03-ba9f-47a9-9457-bc5a837dfa83</td>\n",
       "      <td>med_research</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Eric R. Blough 1,2,3,4,5 and MiaozongWu 1,2,3,...</td>\n",
       "      <td>de38d481-f0a4-44a7-8b88-864fc103ac4f</td>\n",
       "      <td>med_research</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>675</th>\n",
       "      <td>Acetaminophen normalizes glucose\\n</td>\n",
       "      <td>e2adaa00-d883-4a70-8419-581aeaa74170</td>\n",
       "      <td>med_research</td>\n",
       "      <td>675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>676</th>\n",
       "      <td>G. L., Wehner, P., Mangiarua, E.\\n</td>\n",
       "      <td>7dd20ebe-d9af-43e4-b61a-cb44712619f9</td>\n",
       "      <td>med_research</td>\n",
       "      <td>676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>677</th>\n",
       "      <td>S., Karkala, S. K., Rice, K. M., conditions ar...</td>\n",
       "      <td>937a9a4d-9a76-4e06-9a88-e01c72bf848a</td>\n",
       "      <td>med_research</td>\n",
       "      <td>677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>678</th>\n",
       "      <td>Frontiers in Pharmacology | Experimental Pharm...</td>\n",
       "      <td>c570842d-77d7-43e4-83c9-0b8f8f1047bf</td>\n",
       "      <td>med_research</td>\n",
       "      <td>678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>679</th>\n",
       "      <td>November 2011 | Volume 2 | Article 72 | 6</td>\n",
       "      <td>8117faaf-c4fd-4e18-9f6d-026f3f70b043</td>\n",
       "      <td>med_research</td>\n",
       "      <td>679</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>680 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  \\\n",
       "0                                MINI REVIEW ARTICLE\\n   \n",
       "1                        published: 09 November 2011\\n   \n",
       "2                      doi: 10.3389/fphar.2011.00072\\n   \n",
       "3     Acetaminophen: beyond pain and fever-relieving\\n   \n",
       "4    Eric R. Blough 1,2,3,4,5 and MiaozongWu 1,2,3,...   \n",
       "..                                                 ...   \n",
       "675                 Acetaminophen normalizes glucose\\n   \n",
       "676                 G. L., Wehner, P., Mangiarua, E.\\n   \n",
       "677  S., Karkala, S. K., Rice, K. M., conditions ar...   \n",
       "678  Frontiers in Pharmacology | Experimental Pharm...   \n",
       "679          November 2011 | Volume 2 | Article 72 | 6   \n",
       "\n",
       "                                  text_id    request_id  sequence_number  \n",
       "0    e951f600-99e5-499c-8e45-78b87ad76291  med_research                0  \n",
       "1    39e9fc72-cab9-48a2-8c92-fbca710d0e0a  med_research                1  \n",
       "2    d4025a7c-18ba-46d0-8659-356942573265  med_research                2  \n",
       "3    f9092f03-ba9f-47a9-9457-bc5a837dfa83  med_research                3  \n",
       "4    de38d481-f0a4-44a7-8b88-864fc103ac4f  med_research                4  \n",
       "..                                    ...           ...              ...  \n",
       "675  e2adaa00-d883-4a70-8419-581aeaa74170  med_research              675  \n",
       "676  7dd20ebe-d9af-43e4-b61a-cb44712619f9  med_research              676  \n",
       "677  937a9a4d-9a76-4e06-9a88-e01c72bf848a  med_research              677  \n",
       "678  c570842d-77d7-43e4-83c9-0b8f8f1047bf  med_research              678  \n",
       "679  8117faaf-c4fd-4e18-9f6d-026f3f70b043  med_research              679  \n",
       "\n",
       "[680 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# input_data = pd.read_excel(\"../nova/Datasets/Western Wear/western_wear.xlsx\")\n",
    "# input_data[\"request_id\"] = \"western_wear\"\n",
    "\n",
    "input_data = pd.read_excel(\"./Datasets/med_research_transcript.xlsx\")\n",
    "input_data[\"request_id\"] = \"med_research\"\n",
    "\n",
    "input_data[\"sequence_number\"] = range(len(input_data))\n",
    "\n",
    "input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-19 09:25:18,505 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cuda\n",
      "2024-08-19 09:25:18,506 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: paraphrase-MiniLM-L6-v2\n",
      "/home/aamir_syed/nova/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "2024-08-19 09:25:22,548 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cuda\n",
      "2024-08-19 09:25:22,549 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: paraphrase-MiniLM-L6-v2\n",
      "/home/aamir_syed/nova/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "2024-08-19 09:25:24,696 - root - INFO - Building a graph-index using LLMs for 680 documents ...\n",
      "2024-08-19 09:25:24,702 - root - INFO - Chunked 680 documents into 680 chunks ...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using None quantization ...\n",
      "\n",
      "WARNING 08-19 09:25:25 config.py:1086] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 08-19 09:25:25 llm_engine.py:100] Initializing an LLM engine (v0.4.2) with config: model='microsoft/Phi-3-mini-4k-instruct', speculative_config=None, tokenizer='microsoft/Phi-3-mini-4k-instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=microsoft/Phi-3-mini-4k-instruct)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-19 09:25:25 utils.py:660] Found nccl from library /home/aamir_syed/.config/vllm/nccl/cu12/libnccl.so.2.18.1\n",
      "INFO 08-19 09:25:26 selector.py:69] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 08-19 09:25:26 selector.py:32] Using XFormers backend.\n",
      "INFO 08-19 09:25:28 weight_utils.py:199] Using model weights format ['*.safetensors']\n",
      "INFO 08-19 09:25:30 model_runner.py:175] Loading model weights took 7.1183 GB\n",
      "INFO 08-19 09:25:31 gpu_executor.py:114] # GPU blocks: 329, # CPU blocks: 682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aamir_syed/nova/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Processed prompts:  38%|███▊      | 259/680 [03:29<03:25,  2.04it/s]"
     ]
    }
   ],
   "source": [
    "graph_index_builder = GraphIndexRetriever()\n",
    "\n",
    "graph_index_builder.build_index(input_data[\"text\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#just to see what the documents talk about\n",
    "graph_index_builder.graph_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.custom_llm import CustomLLMBuilder\n",
    "\n",
    "def answer_query(query:str, pipeline=graph_index_builder):\n",
    "    \n",
    "    print(query)\n",
    "\n",
    "    relevant_docs = pipeline.retrieve_topk(query=query,\n",
    "                                             topk=5)\n",
    "    relevant_docs = pd.DataFrame(relevant_docs)\n",
    "\n",
    "    print(f\"retrieved {len(relevant_docs)} documents relevant to the query\")\n",
    "    print(f\"{relevant_docs.head()}\\n\")\n",
    "\n",
    "    #get the reasoning for the query\n",
    "    reasoner_query = f\"\"\"you are given a transcript, based on which you are required to reason and answer. here is a summary of the transcript:\\n{pipeline.graph_context}\\n\n",
    "                        given this context, answer any of the questions that follow. your answers must be precise and concise.\"\"\"\n",
    "\n",
    "    with CustomLLMBuilder(system_prompt=reasoner_query,\n",
    "                        model=\"gemini-1.5-flash\") as reasoner:\n",
    "\n",
    "        reasoner_response = reasoner.batch_predict([f\"query : {query}\\n you may use the following information to answer the question : {relevant_docs['document'].unique().tolist()}\",\n",
    "                                                    f\"query : {query}\\n you may use the following information to answer the question : {relevant_docs['node'].unique().tolist()}\"])\n",
    "        \n",
    "        reasoned = {\"full_response\" : reasoner_response[0],\n",
    "                    \"summary_response\" : reasoner_response[1]}\n",
    "    \n",
    "    return reasoned[\"summary_response\"] if reasoned['full_response'] == '' else reasoned['full_response']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_query(\"summarize effects at different dosages of apap\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
